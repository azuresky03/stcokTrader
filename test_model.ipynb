{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### define Temporal Fusion Transformers model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### create time series dataset using data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_forecasting/models/base_model.py:30: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import pytorch_forecasting\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.autonotebook import tqdm\n",
        "from pytorch_forecasting.data import (\n",
        "    TimeSeriesDataSet,\n",
        "    GroupNormalizer\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### for now, we create a continous time index but how to encode the fact that market is not opened on weekend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count     553.000000\n",
            "mean     3308.074977\n",
            "std       377.249733\n",
            "min      2504.389784\n",
            "25%      3063.430361\n",
            "50%      3274.300078\n",
            "75%      3538.379105\n",
            "max      4718.796057\n",
            "Name: next_close, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"./data/Test万华化学.csv\", sep=',')\n",
        "# df_train = df.iloc[:480, :]\n",
        "# df_test = df.iloc[480:, :]\n",
        "df.insert(0, \"time_idx\", [i for i in range(df.shape[0])])\n",
        "df.drop(columns='date', axis=1)\n",
        "df = df.iloc[9:, ]\n",
        "df.insert(7, \"next_close\", df[\"close\"].shift(-1))\n",
        "# df_train = df.iloc[:-1, ]\n",
        "print(df[\"next_close\"].describe())\n",
        "\n",
        "df_train = df.iloc[:544, :]\n",
        "df_test = df.iloc[544:, :]\n",
        "# print(df_test.columns)\n",
        "# print(df_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_prediction_length = 6\n",
        "max_encoder_length = 24\n",
        "\n",
        "training = TimeSeriesDataSet(\n",
        "    data=df_train,\n",
        "    time_idx=\"time_idx\",\n",
        "    target=\"next_close\",\n",
        "    group_ids=[\"code\"],               # only one time series for now\n",
        "    min_encoder_length=0,\n",
        "    min_prediction_length=1,\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    static_categoricals=[\"code\"],\n",
        "    static_reals=[],                 # [\"code\", , \"tradestatus\", \"adjustflag\"],       FIXME: how to solve this\n",
        "    time_varying_unknown_categoricals=[],\n",
        "    time_varying_known_categoricals=[],\n",
        "    time_varying_known_reals=[\"time_idx\"],\n",
        "    time_varying_unknown_reals=[\"open\", \"high\", \"low\", \"close\", \"preclose\", \"volume\", \"amount\", \"turn\", \"pctChg\", \"next_close\"],\n",
        "    variable_groups={},\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups=[\"code\"],\n",
        "        # transformation=\"\"\n",
        "    ),\n",
        "    add_relative_time_idx=True,  # add as feature\n",
        "    add_target_scales=True,  # add as feature\n",
        "    add_encoder_length=True,  # add as feature\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation = TimeSeriesDataSet.from_dataset(\n",
        "    training, df_train, predict=True, stop_randomization=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_dataloader = training.to_dataloader(\n",
        "    train=True, batch_size=batch_size, num_workers=0\n",
        ")\n",
        "val_dataloader = validation.to_dataloader(\n",
        "    train=False, batch_size=batch_size * 10, num_workers=0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pytorch_forecasting.data.timeseries.TimeSeriesDataSet'>\n"
          ]
        }
      ],
      "source": [
        "print(type(training))\n",
        "# print(type(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
            "  rank_zero_warn(\n",
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
            "  rank_zero_warn(\n",
            "\n",
            "   | Name                               | Type                            | Params\n",
            "----------------------------------------------------------------------------------------\n",
            "0  | loss                               | QuantileLoss                    | 0     \n",
            "1  | logging_metrics                    | ModuleList                      | 0     \n",
            "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
            "3  | prescalers                         | ModuleDict                      | 240   \n",
            "4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K \n",
            "5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.2 K \n",
            "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K \n",
            "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
            "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
            "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
            "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
            "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
            "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
            "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
            "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
            "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
            "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
            "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
            "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
            "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
            "20 | output_layer                       | Linear                          | 119   \n",
            "----------------------------------------------------------------------------------------\n",
            "25.4 K    Trainable params\n",
            "0         Non-trainable params\n",
            "25.4 K    Total params\n",
            "0.102     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s, v_num=2, train_loss_step=75.90, val_loss=43.00, train_loss_epoch=76.80]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s, v_num=2, train_loss_step=75.90, val_loss=43.00, train_loss_epoch=76.80]\n",
            "Number of parameters in network: 25.4k\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_forecasting.metrics import QuantileLoss\n",
        "from pytorch_forecasting.models import TemporalFusionTransformer\n",
        "# stop training, when loss metric does not improve on validation set\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=1e-4,\n",
        "    patience=10,\n",
        "    verbose=False,\n",
        "    mode=\"min\"\n",
        ")\n",
        "lr_logger = LearningRateMonitor()  # log the learning rate\n",
        "logger = TensorBoardLogger(\"lightning_logs\")  # log to tensorboard\n",
        "# create trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    # gpus=[0],  # train on CPU, use gpus = [0] to run on GPU\n",
        "    gradient_clip_val=0.1,\n",
        "    # early_stop_callback=early_stop_callback,\n",
        "    limit_train_batches=30,  # running validation every 30 batches\n",
        "    # fast_dev_run=True,  # comment in to quickly check for bugs\n",
        "    callbacks=[lr_logger],\n",
        "    logger=logger,\n",
        ")\n",
        "# initialise model\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,  # biggest influence network size\n",
        "    attention_head_size=1,\n",
        "    dropout=0.01,\n",
        "    hidden_continuous_size=8,\n",
        "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    log_interval=10,  # log example every 10 batches\n",
        "    reduce_on_plateau_patience=4,  # reduce learning automatically\n",
        ")\n",
        "tft.size() # 29.6k parameters in model\n",
        "# fit network\n",
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # find optimal learning rate\n",
        "# res = trainer.tuner.lr_find(\n",
        "#     tft,\n",
        "#     train_dataloader=train_dataloader,\n",
        "#     val_dataloaders=val_dataloader,\n",
        "#     max_lr=0.1,\n",
        "#     min_lr=1e-7,\n",
        "# )\n",
        "\n",
        "# print(f\"suggested learning rate: {res.suggestion()}\")\n",
        "# fig = res.plot(show=True, suggest=True)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
            "  rank_zero_warn(\n",
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
            "  rank_zero_warn(\n",
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        }
      ],
      "source": [
        "from pytorch_forecasting.metrics import MAE\n",
        "# print(val_dataloader)\n",
        "# load the best model according to the validation loss (given that\n",
        "# we use early stopping, this is not necessarily the last epoch)\n",
        "best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
        "# calculate mean absolute error on validation set\n",
        "\n",
        "\n",
        "# actuals = torch.cat([y for x, y in iter(val_dataloader)])\n",
        "actuals = torch.cat([y[0] for _, y in iter(val_dataloader)])  # adjust index as needed\n",
        "# print(actuals)\n",
        "predictions = best_tft.predict(val_dataloader)\n",
        "mae = MAE()\n",
        "mae.update(predictions, actuals)\n",
        "eval_result = mae.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3175.2180, 3201.5142, 3222.9917, 3239.9805, 3254.3284, 3267.1741])\n",
            "tensor([3204.1243, 3192.4854, 3153.4607, 3146.6143, 3148.6682, 3329.7561])\n"
          ]
        }
      ],
      "source": [
        "print(predictions[0])\n",
        "print(actuals[0])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert tensor to a dataloader as new input to put into our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(61.5124)\n"
          ]
        }
      ],
      "source": [
        "print(eval_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count       9.000000\n",
            "mean     3210.286057\n",
            "std        80.740956\n",
            "min      3106.220482\n",
            "25%      3155.514701\n",
            "50%      3200.701069\n",
            "75%      3238.356376\n",
            "max      3359.195679\n",
            "Name: next_close, dtype: float64\n",
            "5\n",
            "tensor([[3198.8347, 3232.7832, 3255.1440, 3272.1821, 3286.2891, 3298.4563]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/feiyangm/.conda/envs/CS175/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[3198.8347, 3232.7832, 3255.144 , 3272.1821, 3286.289 , 3298.4563]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# df_test=df_test.drop([\"next_close\"],axis=1)\n",
        "df_test = df_test.iloc[:-1, ]\n",
        "print(df_test[\"next_close\"].describe())\n",
        "\n",
        "# df_test=df_test.drop([\"next_close\"],axis=1)\n",
        "test = TimeSeriesDataSet.from_dataset(\n",
        "    training, df_test, predict=True, stop_randomization=True\n",
        ")\n",
        "test_dataloader = test.to_dataloader(\n",
        "    train=False, batch_size=batch_size * 10, num_workers=0\n",
        ")\n",
        "pred=best_tft.predict(df_test,return_x=True)\n",
        "print(len(pred))\n",
        "print(pred[0])\n",
        "pred_numpy = pred[0].detach().numpy()\n",
        "pred_numpy\n",
        "# plt.plot(pred_numpy)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[3238.3564, 3165.7844, 3106.2205, 3226.3752, 3140.1104, 3200.7012]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(38.2260)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actuals_test = torch.cat([y[0] for _, y in iter(test_dataloader)])  # adjust index as needed\n",
        "# actuals_test = torch.cat(df_test['next_close'].tolist())\n",
        "# numpy_array = df_test['next_close'].to_numpy()\n",
        "# tensor = torch.from_numpy(numpy_array)\n",
        "print(actuals_test)\n",
        "mae1 = MAE()\n",
        "mae1.update(pred[0], actuals_test)\n",
        "eval_result = mae1.compute()\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_forecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m SMAPE\n\u001b[1;32m      2\u001b[0m \u001b[39m# calculate metric by which to display\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m predictions, x \u001b[39m=\u001b[39m best_tft\u001b[39m.\u001b[39mpredict(val_dataloader)\n\u001b[1;32m      4\u001b[0m mean_losses \u001b[39m=\u001b[39m SMAPE(reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m)(predictions, actuals)\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m indices \u001b[39m=\u001b[39m mean_losses\u001b[39m.\u001b[39margsort(descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# sort losses\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ],
      "source": [
        "# from pytorch_forecasting.metrics import SMAPE\n",
        "# # calculate metric by which to display\n",
        "# predictions, x = best_tft.predict(val_dataloader)\n",
        "# mean_losses = SMAPE(reduction=\"none\")(predictions, actuals).mean(1)\n",
        "# indices = mean_losses.argsort(descending=True)  # sort losses\n",
        "# raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw, return_x=True\")\n",
        "# # show only two examples for demonstration purposes\n",
        "# for idx in range(2):\n",
        "#     best_tft.plot_prediction(\n",
        "#         x,\n",
        "#         raw_predictions,\n",
        "#         idx=indices[idx],\n",
        "#         add_loss_to_title=SMAPE()\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'raw_predictions' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m interpretation \u001b[39m=\u001b[39m best_tft\u001b[39m.\u001b[39minterpret_output(\n\u001b[0;32m----> 2\u001b[0m     raw_predictions, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m best_tft\u001b[39m.\u001b[39mplot_interpretation(interpretation)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_predictions' is not defined"
          ]
        }
      ],
      "source": [
        "# interpretation = best_tft.interpret_output(\n",
        "#     raw_predictions, reduction=\"sum\"\n",
        "# )\n",
        "# best_tft.plot_interpretation(interpretation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
